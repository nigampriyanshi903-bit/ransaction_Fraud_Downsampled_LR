{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dea42a3-6f0e-45e9-b908-a4d4c28e4ae8",
   "metadata": {},
   "source": [
    "## Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49187e1-55ed-4d03-aff3-facade996138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Data Loading ---\n",
      " Data file successfully loaded. Total transactions: 284807\n",
      "\n",
      "--- Initial Inspection (Head & Info) ---\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n",
      "\n",
      "--- Target Variable Distribution (Class) ---\n",
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fraudulent transactions (Class 1): 492\n",
      "Percentage of Fraud: 0.172749%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\Users\\Asus\\Downloads\\creditcard.csv\\creditcard.csv\"\n",
    "\n",
    "print(\"--- Step 1: Data Loading ---\")\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\" Data file successfully loaded. Total transactions: {len(df)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: File not found at '{file_path}'. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Initial Inspection\n",
    "print(\"\\n--- Initial Inspection (Head & Info) ---\")\n",
    "print(df.head())\n",
    "print(df.info()) \n",
    "\n",
    "# 3. Target Variable Distribution (Class)\n",
    "print(\"\\n--- Target Variable Distribution (Class) ---\")\n",
    "# 'Class' (0: Legitimate, 1: Fraud) is the target variable\n",
    "class_counts = df['Class'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Calculate the imbalance\n",
    "fraud_count = class_counts.get(1, 0) # Use .get(1, 0) in case no fraud exists (unlikely)\n",
    "total_count = len(df)\n",
    "fraud_percentage = (fraud_count / total_count) * 100\n",
    "\n",
    "print(f\"\\nFraudulent transactions (Class 1): {fraud_count}\")\n",
    "print(f\"Percentage of Fraud: {fraud_percentage:.6f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748eef1-878d-4ced-b2b9-548c2b4427f2",
   "metadata": {},
   "source": [
    "## Data Preprocessing (Scaling and Feature Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903aa36-a69f-4d6e-a4e5-bf2027552884",
   "metadata": {},
   "source": [
    "#### 1. Code for Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be35e583-9c1a-4dcd-ad97-7c3de08fe3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time and Amount features successfully scaled using RobustScaler.\n",
      "\n",
      "--- Processed Data Head (Showing scaled features) ---\n",
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
      "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
      "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
      "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
      "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
      "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
      "\n",
      "        V25       V26       V27       V28  scaled_amount  scaled_time  \n",
      "0  0.128539 -0.189115  0.133558 -0.021053       1.783274    -0.994983  \n",
      "1  0.167170  0.125895 -0.008983  0.014724      -0.269825    -0.994983  \n",
      "2 -0.327642 -0.139097 -0.055353 -0.059752       4.983721    -0.994972  \n",
      "3  0.647376 -0.221929  0.062723  0.061458       1.418291    -0.994972  \n",
      "4 -0.206010  0.502292  0.219422  0.215153       0.670579    -0.994960  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# Anomaly Detection mein hum RobustScaler ko prefer karte hain, \n",
    "# kyunki yeh outliers (jo ki Fraud transactions hi hain) se kam affect hota hai.\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Features to be scaled: Time and Amount\n",
    "df['scaled_amount'] = robust_scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "df['scaled_time'] = robust_scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "\n",
    "# Drop original Time and Amount columns\n",
    "df = df.drop(['Time', 'Amount'], axis=1)\n",
    "\n",
    "# Rearrange columns so the target ('Class') is at the end\n",
    "# X = features, y = target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "print(\" Time and Amount features successfully scaled using RobustScaler.\")\n",
    "print(\"\\n--- Processed Data Head (Showing scaled features) ---\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb96b2e-2985-4876-bdc4-f3d7a5552bd3",
   "metadata": {},
   "source": [
    "#### 2. Isolation Forest Setup (One-Class Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92d586f-2937-4c39-9275-a68edf0fca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Isolation Forest Training Data size (Legitimate only): 284315\n"
     ]
    }
   ],
   "source": [
    "# Separate Legitimate and Fraudulent transactions\n",
    "df_legitimate = df[df['Class'] == 0]\n",
    "df_fraud = df[df['Class'] == 1]\n",
    "\n",
    "# We will only train Isolation Forest on the legitimate data (Unsupervised Anomaly Detection)\n",
    "X_train_isolation = df_legitimate.drop('Class', axis=1)\n",
    "\n",
    "print(f\"\\nIsolation Forest Training Data size (Legitimate only): {len(X_train_isolation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68773dc6-3385-4d24-9416-4f28219389b0",
   "metadata": {},
   "source": [
    "## Model Training (Isolation Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0feded93-9db7-47be-945f-353b1579bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Isolation Forest Model (Unsupervised)...\n",
      " Isolation Forest Model successfully trained.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Contamination: Yeh parameter batata hai ki hum kitne percentage data ko anomaly (fraud) expect karte hain.\n",
    "# Hum iska value actual fraud percentage (0.172749%) ke aas-paas set karenge.\n",
    "contamination_rate = 0.0017275  # ~0.17275%\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "# random_state=42 for reproducibility\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100, \n",
    "    max_samples='auto', \n",
    "    contamination=contamination_rate, \n",
    "    random_state=42, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train the model ONLY on the legitimate transactions (X_train_isolation)\n",
    "print(\"\\nTraining Isolation Forest Model (Unsupervised)...\")\n",
    "iso_forest.fit(X_train_isolation)\n",
    "\n",
    "print(\" Isolation Forest Model successfully trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a707a49-d545-4698-85b3-fe9b43f0ea95",
   "metadata": {},
   "source": [
    "## Prediction & Evaluation (Isolation Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346309d5-dbb6-470a-ac04-b4f1d59b7066",
   "metadata": {},
   "source": [
    "#### 1. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de0dd68-f804-477f-8b49-e5dab336a62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on the entire dataset...\n",
      " Predictions completed and mapped to 0/1.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict anomalies (frauds) on the entire dataset (X)\n",
    "# X contains all transactions (Legitimate and Fraud)\n",
    "print(\"Making predictions on the entire dataset...\")\n",
    "y_pred_iso = iso_forest.predict(X)\n",
    "\n",
    "# Map the Isolation Forest output (1: Inlier, -1: Outlier) to our target labels (0: Legitimate, 1: Fraud)\n",
    "# Fraud (1) is represented by -1 (Outlier) in Isolation Forest.\n",
    "y_pred_mapped = np.where(y_pred_iso == -1, 1, 0) \n",
    "\n",
    "print(\" Predictions completed and mapped to 0/1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01def2ba-c5fc-4010-9a58-0f7ca170972e",
   "metadata": {},
   "source": [
    "#### 2. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc39804-e2e1-4edb-8fad-490d8ccc1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Confusion Matrix ---\n",
      "[[283823    492]\n",
      " [   357    135]]\n",
      "\n",
      "--- Classification Report (Focus on Class 1: Fraud) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    284315\n",
      "           1       0.22      0.27      0.24       492\n",
      "\n",
      "    accuracy                           1.00    284807\n",
      "   macro avg       0.61      0.64      0.62    284807\n",
      "weighted avg       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Confusion Matrix\n",
    "cm_iso = confusion_matrix(y, y_pred_mapped)\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "print(cm_iso)\n",
    "\n",
    "# Generate the detailed Classification Report\n",
    "print(\"\\n--- Classification Report (Focus on Class 1: Fraud) ---\")\n",
    "print(classification_report(y, y_pred_mapped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ecc2e-65c1-45ea-a113-23dd928004c6",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a379ca6d-2945-4f1c-b735-faf11b63d064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying SMOTE to training data...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 52.1 MiB for an array with shape (30, 227845) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mApplying SMOTE to training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m sm \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m X_train_res, y_train_res \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Training Data size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResampled Training Data size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train_res)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\base.py:99\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m     97\u001b[0m check_classification_targets(y)\n\u001b[0;32m     98\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m---> 99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\base.py:157\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    155\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    156\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 157\u001b[0m X, y \u001b[38;5;241m=\u001b[39m validate_data(\u001b[38;5;28mself\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2969\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2970\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2971\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2972\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1363\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1364\u001b[0m     )\n\u001b[0;32m   1366\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1368\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1369\u001b[0m     X,\n\u001b[0;32m   1370\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1371\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1372\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1373\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1374\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1375\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[0;32m   1376\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39mensure_all_finite,\n\u001b[0;32m   1377\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1378\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1379\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1380\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1381\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1382\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1383\u001b[0m )\n\u001b[0;32m   1385\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1387\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:1053\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1056\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1057\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_array_api.py:757\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    755\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 757\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    759\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:2152\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\n\u001b[0;32m   2150\u001b[0m     \u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, copy: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2152\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   2153\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   2154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2155\u001b[0m         astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   2156\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2157\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block\n\u001b[0;32m   2158\u001b[0m     ):\n\u001b[0;32m   2159\u001b[0m         \u001b[38;5;66;03m# Check if both conversions can be done without a copy\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1127\u001b[0m, in \u001b[0;36mDataFrame._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m blocks \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m   1129\u001b[0m arr \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:12664\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12590\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m  12591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m  12592\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  12593\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  12594\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12662\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  12663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 12664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mas_array()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interleave(dtype\u001b[38;5;241m=\u001b[39mdtype, na_value\u001b[38;5;241m=\u001b[39mna_value)\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1727\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[0;32m   1726\u001b[0m dtype \u001b[38;5;241m=\u001b[39m ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m-> 1727\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1729\u001b[0m itemmask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 52.1 MiB for an array with shape (30, 227845) and data type float64"
     ]
    }
   ],
   "source": [
    "# Hum is data ko supervised model jaise Logistic Regression ke saath test karenge,\n",
    "# lekin hum imbalance ko handle karne ke liye SMOTE ya Class Weighting ka use karenge.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE # Requires installation: pip install imblearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Data Splitting (Using all features X)\n",
    "# X (V1-V28, scaled_time, scaled_amount), y (Class)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y # Imbalance ko train/test sets mein maintain karein\n",
    ")\n",
    "\n",
    "# 2. Apply SMOTE to the training data ONLY to balance the classes\n",
    "print(\"\\nApplying SMOTE to training data...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original Training Data size: {len(X_train)}\")\n",
    "print(f\"Resampled Training Data size: {len(X_train_res)}\")\n",
    "print(f\"Resampled Class Distribution:\\n{y_train_res.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc94d2d-a8a8-4728-bfa4-315fc4f2a2cb",
   "metadata": {},
   "source": [
    "## Downsampling and Supervised Training (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a011892-8e7e-4183-840b-5b3278878ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Legitimate data successfully downsampled to 2460 samples.\n",
      "Total Combined Balanced Data Size: 2952\n",
      "Balanced Data Class Distribution:\n",
      "Class\n",
      "0    2460\n",
      "1     492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Separate Legitimate and Fraudulent transactions (using the variables created earlier)\n",
    "# df_legitimate (284315 samples), df_fraud (492 samples)\n",
    "\n",
    "# 1. Downsample Legitimate Transactions (Class 0)\n",
    "# Target ratio: Let's aim for 5:1 ratio (Fraud: 492, Legitimate: 492 * 5 = 2460)\n",
    "legit_sample_size = len(df_fraud) * 5 # 492 * 5 = 2460 samples\n",
    "\n",
    "# Randomly select a smaller subset of legitimate transactions\n",
    "df_legitimate_sampled = df_legitimate.sample(n=legit_sample_size, random_state=42)\n",
    "\n",
    "# 2. Combine the sampled legitimate data and all fraud data\n",
    "df_balanced = pd.concat([df_legitimate_sampled, df_fraud], ignore_index=True)\n",
    "\n",
    "print(f\" Legitimate data successfully downsampled to {legit_sample_size} samples.\")\n",
    "print(f\"Total Combined Balanced Data Size: {len(df_balanced)}\")\n",
    "print(f\"Balanced Data Class Distribution:\\n{df_balanced['Class'].value_counts()}\")\n",
    "\n",
    "# 3. Final Split (Downsampled)\n",
    "X_balanced = df_balanced.drop('Class', axis=1)\n",
    "y_balanced = df_balanced['Class']\n",
    "\n",
    "# Split the balanced data into Training and Testing sets\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(\n",
    "    X_balanced, y_balanced, \n",
    "    test_size=0.3, # Using a standard 70/30 split on the balanced data\n",
    "    random_state=42, \n",
    "    stratify=y_balanced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55005585-7926-4386-b721-648004bd7ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression Model on Downsampled Data...\n",
      " Logistic Regression Model successfully trained.\n"
     ]
    }
   ],
   "source": [
    "#Supervised Model Training (Logistic Regression)\n",
    "# Initialize and Train Logistic Regression on the downsampled data\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "print(\"\\nTraining Logistic Regression Model on Downsampled Data...\")\n",
    "lr_model.fit(X_train_bal, y_train_bal)\n",
    "print(\" Logistic Regression Model successfully trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1459d-4b1a-43bd-bfc5-c1b2a14cb3c1",
   "metadata": {},
   "source": [
    "## Evaluation on the Original, Unbalanced Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2735bec4-6d91-4f66-bc68-10639516aa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test Set Size: 56962\n",
      "Original Test Set Fraud Count: 98\n"
     ]
    }
   ],
   "source": [
    "# Hum Step 5 se yahi split use karenge. Original data se 20% test set\n",
    "# Original X (poore features) aur y (Class) ko use karte hue.\n",
    "\n",
    "# Original Data Splitting (20% for testing)\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X aur y variable Step 2 mein define kiye gaye the.\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y # Imbalance ko maintain karein\n",
    ")\n",
    "\n",
    "print(f\"Original Test Set Size: {len(X_test_orig)}\")\n",
    "print(f\"Original Test Set Fraud Count: {y_test_orig.value_counts()[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e5bba58-cd08-4702-8c71-607304c0c293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making predictions on the ORIGINAL, UNBALANCED Test Set...\n",
      " Predictions completed.\n"
     ]
    }
   ],
   "source": [
    "#1. Prediction'\n",
    "# Use the trained Logistic Regression model (lr_model)\n",
    "# to predict on the Original Test Set (X_test_orig)\n",
    "print(\"\\nMaking predictions on the ORIGINAL, UNBALANCED Test Set...\")\n",
    "y_pred_lr = lr_model.predict(X_test_orig)\n",
    "\n",
    "print(\" Predictions completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46aecf95-b539-4c93-8f85-f9dbc5e1b7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Confusion Matrix (Logistic Regression) ---\n",
      "[[56377   487]\n",
      " [   10    88]]\n",
      "\n",
      "--- Classification Report (Focus on Class 1: Fraud) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     56864\n",
      "           1       0.15      0.90      0.26        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.58      0.94      0.63     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n",
      "Overall Accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "#2. Evaluation Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Calculate Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test_orig, y_pred_lr)\n",
    "print(\"\\n--- Confusion Matrix (Logistic Regression) ---\")\n",
    "print(cm_lr)\n",
    "\n",
    "# Generate the detailed Classification Report\n",
    "print(\"\\n--- Classification Report (Focus on Class 1: Fraud) ---\")\n",
    "print(classification_report(y_test_orig, y_pred_lr))\n",
    "\n",
    "# Calculate Accuracy (just for reporting)\n",
    "accuracy_lr = accuracy_score(y_test_orig, y_pred_lr)\n",
    "print(f\"Overall Accuracy: {accuracy_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266e00d-283f-43da-a2ab-76028cd4b21e",
   "metadata": {},
   "source": [
    "##  Project Conclusion: Downsampled Logistic Regression (Supervised)\n",
    "\n",
    "The Credit Card Fraud Detection project explored both unsupervised (Isolation Forest) and supervised (Logistic Regression with Downsampling) methods on a highly imbalanced dataset (0.17% Fraud).\n",
    "\n",
    "### Key Findings and Model Selection\n",
    "\n",
    "1.  **Isolation Forest (Unsupervised)**: Performed poorly, achieving a low **Recall of $0.27$** (missing 73% of actual fraud cases), making it unsuitable for a production environment.\n",
    "\n",
    "2.  **Logistic Regression (Downsampled Supervised)**:\n",
    "    * By downsampling the majority class (Legitimate) to a 5:1 ratio, the model was trained effectively to identify patterns in the rare Fraud class.\n",
    "    * It achieved a critical **Recall of $0.90$**, correctly identifying 88 out of 98 fraud transactions in the test set. This drastically reduces **False Negatives** (missed fraud) to just **10**.\n",
    "    * **Trade-off:** This high Recall came at the cost of **Precision ($0.15$)**, resulting in 487 **False Positives** (legitimate transactions incorrectly flagged as fraud).\n",
    "\n",
    "### Final Recommendation\n",
    "\n",
    "The **Logistic Regression (Downsampled)** model is the preferred solution. In financial security, **maximizing Recall (catching fraud)** is typically prioritized over maximizing Precision, as the cost of a missed fraud case is generally higher than the inconvenience of flagging a legitimate transaction (which can be manually verified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17954b7-128b-42c5-a2c4-3d706adbde3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
